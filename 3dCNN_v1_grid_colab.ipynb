{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOpUTcW/EobBZcVP0GBwHcv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghess/607-sensory-coding/blob/main/3dCNN_v1_grid_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile as tiff\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "E7q4MU6943Kz"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl86lWdq3lg4",
        "outputId": "6220dcc3-4cd3-48fb-dbae-723e1c137419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/607_sensory_coding\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the repository's directory\n",
        "repo_path = os.path.join(path, '607-sensory-coding')\n",
        "os.chdir(repo_path)"
      ],
      "metadata": {
        "id": "w0XPDO0q4uuU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NqBblxdMJMcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset: Tiff stack with two labels\n",
        "class TiffDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        "        image = tiff.imread(img_name)  # Load stack\n",
        "\n",
        "        # Manually convert the image to a tensor and normalize\n",
        "        image = torch.from_numpy(image).type(torch.FloatTensor) / 255.0\n",
        "\n",
        "        # Add channel dim\n",
        "        image = image.unsqueeze(0)\n",
        "\n",
        "        #if self.transform:\n",
        "        #    image = self.transform(image)\n",
        "\n",
        "        # Ensure the image is a float tensor\n",
        "        if not isinstance(image, torch.FloatTensor):\n",
        "            image = image.type(torch.FloatTensor)\n",
        "\n",
        "        # Convert labels to numerical format\n",
        "        rotation_class = 1 if self.annotations.iloc[index, 1] == 'clockwise' else 0\n",
        "        angle_class = int(self.annotations.iloc[index, 2])  # Maybe use int()\n",
        "\n",
        "        # Combine the labels (e.g., using one-hot encoding for the input class)\n",
        "        label = torch.tensor([rotation_class, angle_class], dtype=torch.long)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define a custom transform\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     lambda x: x.unsqueeze(0)  # Add a channel dimension\n",
        "# ])\n",
        "\n",
        "dataset = TiffDataset(csv_file='dataset/slice/labels_slice.csv', root_dir='dataset/')\n",
        "\n",
        "# Determine the lengths for train and test sets\n",
        "train_size = int(0.6 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=360, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=360, shuffle=False)"
      ],
      "metadata": {
        "id": "PQJFLUdP5CsD"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the first few samples\n",
        "for i in range(5):\n",
        "    image, label = dataset.__getitem__(i)\n",
        "    print(f\"Image shape for sample {i}:\", image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsUy-pW7PnZ4",
        "outputId": "66d83882-5304-4c85-fe98-e2b0a79a715a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape for sample 0: torch.Size([1, 5, 128, 128])\n",
            "Image shape for sample 1: torch.Size([1, 5, 128, 128])\n",
            "Image shape for sample 2: torch.Size([1, 5, 128, 128])\n",
            "Image shape for sample 3: torch.Size([1, 5, 128, 128])\n",
            "Image shape for sample 4: torch.Size([1, 5, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model"
      ],
      "metadata": {
        "id": "v6Nx9f6LJH2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv3d(16, 16, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(16 * 32 * 1 * 32, 512)\n",
        "        self.fc_rotation = nn.Linear(512, 2)  # 2 classes for rotation\n",
        "        self.fc_orientation = nn.Linear(512, 36)  # 36 classes for orientation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Apply 3D convolution and pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Apply 3D convolution and pooling\n",
        "        x = x.view(-1, 16 * 32 * 1 * 32)  # Flatten tensor\n",
        "        x = F.relu(self.fc1(x))  # Fully connected layer\n",
        "        rotation_output = self.fc_rotation(x)\n",
        "        orientation_output = self.fc_orientation(x)\n",
        "        return rotation_output, orientation_output\n",
        "\n",
        "model = Simple3DCNN()"
      ],
      "metadata": {
        "id": "7KNQIvYL5F2d"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "9jU428QFJDEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# Initialize a list to keep track of loss values\n",
        "loss_values = []\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is active.\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device (GPU or CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "        # Move to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Split the labels\n",
        "        rotation_labels = labels[:, 0]\n",
        "        input_labels = labels[:, 1]\n",
        "\n",
        "        # Reset gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Assuming your model's output is designed to handle both types of labels\n",
        "        loss_rotation = criterion(outputs[0], rotation_labels)\n",
        "        loss_input = criterion(outputs[1], input_labels)\n",
        "        loss = loss_rotation + loss_input  # Combine losses, or handle as you see fit\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # At the end of each epoch, store the average loss\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    loss_values.append(epoch_loss)\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "id": "lsFj5-Gr5I57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4aa6ed-638c-4377-81de-961a0bbacda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is active.\n",
            "Epoch [1/100], Loss: 4.2765\n",
            "Epoch [2/100], Loss: 4.2766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model to File"
      ],
      "metadata": {
        "id": "TwGTSrcXJxO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "sjsQE4kTDXKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model, '/content/drive/My Drive/607_sensory_coding/test_model_2.pt')"
      ],
      "metadata": {
        "id": "DBAmHUoS77Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "_RpXvKiMJz7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test label prediction performance\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct_rotation, correct_input, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            rotation_labels, input_labels = labels[:, 0], labels[:, 1]\n",
        "\n",
        "            rotation_output, input_output = model(inputs)\n",
        "\n",
        "            _, predicted_rotation = torch.max(rotation_output.data, 1)\n",
        "            _, predicted_input = torch.max(input_output.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct_rotation += (predicted_rotation == rotation_labels).sum().item()\n",
        "            correct_input += (predicted_input == input_labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on rotation prediction: {100 * correct_rotation / total}%')\n",
        "    print(f'Accuracy of the network on input type prediction: {100 * correct_input / total}%')\n",
        "\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "AoFmssCl5Ndf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Trained Layers"
      ],
      "metadata": {
        "id": "P0ZOW93HWr2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pull a single sample input out for layer visualization\n",
        "def get_sample_input(data_loader):\n",
        "\n",
        "    for inputs, _ in data_loader:\n",
        "        # Select one instance in the batch\n",
        "        sample_input = inputs[0]\n",
        "        return sample_input\n",
        "\n",
        "sample_input = get_sample_input(test_loader)"
      ],
      "metadata": {
        "id": "W8GUrGWX86pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_maps(model, input_tensor, selected_layers, ncols=4):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Function to get the output of a layer\n",
        "    def get_features_map(layer, input, output):\n",
        "        feature_maps.append(output.cpu().data.numpy())\n",
        "\n",
        "    # Attach hooks to the selected layers\n",
        "    hooks = []\n",
        "    for name, layer in model.named_modules():\n",
        "        if name in selected_layers:\n",
        "            hooks.append(layer.register_forward_hook(get_features_map))\n",
        "\n",
        "    # Initialize the feature maps list and pass the input through the model\n",
        "    feature_maps = []\n",
        "    with torch.no_grad():\n",
        "        model(input_tensor.unsqueeze(0).to(device))\n",
        "\n",
        "    # Remove hooks (important to avoid memory leak)\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Plotting\n",
        "    for ii in range(1):\n",
        "      for layer_maps in feature_maps:\n",
        "          n_features = layer_maps.shape[1]\n",
        "          nrows = n_features // ncols + int(n_features % ncols != 0)\n",
        "          fig, axes = plt.subplots(nrows, ncols, figsize=(20, 2 * nrows))\n",
        "          for i in range(n_features):\n",
        "              row = i // ncols\n",
        "              col = i % ncols\n",
        "              ax = axes[row, col] if nrows > 1 else axes[col]\n",
        "              print(layer_maps.shape)\n",
        "              #layers_maps\n",
        "              ax.imshow(layer_maps[ii, i, 0, :], cmap='gray')\n",
        "              ax.axis('off')\n",
        "\n",
        "          plt.show()\n",
        "\n",
        "          # Save the figure\n",
        "          #plt.savefig(f\"grid_epoch_{epochs}_conv2[{ii}].png\")  # Saves the plot as a PNG file\n",
        "          #plt.close()  # Closes the current figure\n",
        "\n",
        "# Pull out a layer and plot slices\n",
        "selected_layers = ['conv2']\n",
        "plot_feature_maps(model, sample_input, selected_layers, ncols=6)\n"
      ],
      "metadata": {
        "id": "br1Ww3pz8oLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close('all')"
      ],
      "metadata": {
        "id": "EP7ni4bIri_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dataset[0])\n",
        "# fig, axes = plt.subplots(nrows, ncols, figsize=(20, 2 * nrows))\n",
        "\n",
        "ncols = 5\n",
        "nrows = 7\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 2 * nrows))\n",
        "\n",
        "# Plot a 4x4 grid of images from dataset\n",
        "for i in range(35):\n",
        "    row = i // ncols\n",
        "    col = i % ncols\n",
        "    ax = axes[row, col] if nrows > 1 else axes[col]\n",
        "    # add correspnding label from labels list\n",
        "    #rotation_labels, input_labels = labels[:, 0], labels[:, 1]\n",
        "    ax.set_title(f\"{rotation_labels[0]}, {input_labels[0]}\")\n",
        "    img = dataset[i][0][:]\n",
        "    img = img[:,0,:]\n",
        "    #img = img.reshape(128, 128, 1).squeeze()\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.axis('off')"
      ],
      "metadata": {
        "id": "IKUU1YZyqdTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}