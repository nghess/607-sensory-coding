[
  {
    "objectID": "2dcnn.html",
    "href": "2dcnn.html",
    "title": "Model 2: Looped 2DCNN",
    "section": "",
    "text": "flowchart TD\nStimulus --&gt; 2DConv\n2DConv --&gt; |loop over each frame| 2DConv\n2DConv --&gt; fc1[\"Fully Connected Layer\"]\nfc1 --&gt; fc_r[\"Rotation Output\"]\nfc1 --&gt; fc_o[\"Orientation Output\"]"
  },
  {
    "objectID": "2dcnn.html#motivation",
    "href": "2dcnn.html#motivation",
    "title": "Model 2: Looped 2DCNN",
    "section": "Motivation",
    "text": "Motivation\nOnce I finished the 3DCNN, the project became a balancing act between obtaining good classification results and maintaining a relationship to the primate visual system. I began with the 3DCNN architecture that obtained good classification results, but over the course of the project decided that the kernels learned by the 3D CNN abstracted the problem to a degree that the learned kernels didn’t represent biological receptive fields adequately.\nConsidering that the retina captures 2-dimensional information, I opted to switch to a 2d convolutional architecture and add an LSTM layer separately to keep track of differences between contiguous animation frames. My hope was that this would enable the network to learn kernels that resembled those found in MT+.\nI did get the 2DCNN-LSTM model working, and that was a great leanring experience. Unfortunately LSTM is very compute-hungry, and despite getting a working model, I didn’t have enough time to properly train and test it.\nLuckily, in the course of preparing the 2D CNN output for the LSTM layer, I developed an architecture that allowed 2D CNNs to learn temporal features on their own. I put 2d convolution layers inside a loop, concatenated each layer into an array and flattened the result for each 5-frame stack, then predicted on that output. This architecture is fairly computationally efficient, and more accurately models 2D inputs flowing through the visual stream.\n2DCNN on GitHub"
  },
  {
    "objectID": "2dcnn.html#results",
    "href": "2dcnn.html#results",
    "title": "Model 2: Looped 2DCNN",
    "section": "Results",
    "text": "Results\nLargely due to the difficult orientation classification task, I was unable to get the 2DCNN to perform well. While the kernels show some structure, they are clearly much less refined than those in the 3DCNN (which was used on a simpler task).\n\n\n\nFigure 3: A small selection of kernels learned by the 2DCNN. While there is some emerging structure, it doesn’t capture features of the stimuli well."
  },
  {
    "objectID": "bigrf.html",
    "href": "bigrf.html",
    "title": "Big Receptive Field",
    "section": "",
    "text": "Throughout the project, I wanted to find a way to train a model to learn a convolutional kernel (or collection of kernels) that approximated the sparse-but-uniform structure of orientation-selective cortical columns in MT. As 607 Sensory Coding was about mapping, I was motivated to create a model that “mapped” the entire stimulus space.\nTypically, CNNs learn a number of kernel and then cycle through them as they slide across a stimulus. While the results of this process share properties with biological visual systems, there is no structure retinotopic or otherwise, that fixes a particular kernel to a particular location in feature space.\nIn the course of the project, I had a moment of clarity as to why orientation-selective cortical columns are so nicely interspersed with each other: maximum coverage of the possible feature space. While they are fixed, cortical columns of a particular selectivity span the entire retinotopic map. This coverage is sparser than in a CNN, where every kernel sees every pixel in a stimulus, but the idea is the same.\nBased on this thought, I modified my 3DCNN so that the second convolutional layer had a single large kernel that matched the stimulus size exactly. This kernel never moved across the image, rather it sat in place while the stimuli under it changed.\nThe resulting kernel produced smoothly-transitioning feature detectors, however, I need to reduce the number of channels (dimensions) and train a new model before I can make any comparisons to kernel structure to columns in cortex."
  },
  {
    "objectID": "3dcnn.html",
    "href": "3dcnn.html",
    "title": "Model 1: 3DCNN",
    "section": "",
    "text": "flowchart TD\nStimulus --&gt; 3DConv\n3DConv --&gt; fc1[\"Fully Connected Layer\"]\nfc1 --&gt; fc_r[\"Rotation Output\"]\nfc1 --&gt; fc_o[\"Shape Output\"]"
  },
  {
    "objectID": "3dcnn.html#motivation",
    "href": "3dcnn.html#motivation",
    "title": "Model 1: 3DCNN",
    "section": "Motivation",
    "text": "Motivation\nI started the project using a 3D CNN architecture that obtained good classification results on rotation direction and stimulus shape, but over the course of the project decided that a 3D CNN abstracted the problem to a degree that the learned kernels didn’t represent biological receptive fields adequately. Considering that the retina captures 2-dimensional information, and topological maps in cortex are essentially 2D, I switched to a 2D architecture (2DCNN) that employed looping to train the convolutional layers on the temporal dimension of the stimuli.\n3DCNN on GitHub"
  },
  {
    "objectID": "3dcnn.html#results",
    "href": "3dcnn.html#results",
    "title": "Model 1: 3DCNN",
    "section": "Results",
    "text": "Results\nThe 3DCNN performed very well, and could reach &gt;98% accuracy on both classification tasks in 100 epochs or fewer. While they may not be biomimetic, 3DCNNs do an excellent job learning features of 3-dimensional data.\n\n\n\nFigure 2: A small selection of kernels learned by the 3DCNN. Note that the kernel in row 1, column 2 resembles a classic description of a direction-sensitive center-surround RF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "607 Sensory Coding Final Project",
    "section": "",
    "text": "For my 607 Sensory Coding Project, I took a first stab at building a biologically inspired neural network modeling the visual system. As my primary interest is in the computation and perception of motion, I sought to design an architecture that mimics some of the properties of primate area MT. The models were trained in two tasks: Judgment of object orientation, and judgment object rotation direction. To teach the networks these tasks, I created a synthetic dataset based on an imagined psychophysical task in which the subject would judge rotation direction and average orientation (in degrees) of a short animation of a rotating bar.\nI had two primary goals this project: That my models would learn feature detectors analogous to direction-sensitive neurons in MT, and that I could devise a situation where the network learned features that mimicked the structure of cortical columns.\n\n\n\nFigure 1: Direction-selective Receptive Fields. One goal of this project was to train a convolutional neural network to learn kernels with similar properties to these RFs."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "607 Sensory Coding Final Project",
    "section": "",
    "text": "For my 607 Sensory Coding Project, I took a first stab at building a biologically inspired neural network modeling the visual system. As my primary interest is in the computation and perception of motion, I sought to design an architecture that mimics some of the properties of primate area MT. The models were trained in two tasks: Judgment of object orientation, and judgment object rotation direction. To teach the networks these tasks, I created a synthetic dataset based on an imagined psychophysical task in which the subject would judge rotation direction and average orientation (in degrees) of a short animation of a rotating bar.\nI had two primary goals this project: That my models would learn feature detectors analogous to direction-sensitive neurons in MT, and that I could devise a situation where the network learned features that mimicked the structure of cortical columns.\n\n\n\nFigure 1: Direction-selective Receptive Fields. One goal of this project was to train a convolutional neural network to learn kernels with similar properties to these RFs."
  },
  {
    "objectID": "index.html#successes",
    "href": "index.html#successes",
    "title": "607 Sensory Coding Final Project",
    "section": "Successes",
    "text": "Successes\nThe networks were able to learn kernels that mimic the propery of direction-selective neurons, and I was able to generate a crude approximation of columnar structure by training a network that used single large kernels to convolved over the input, rather than small kernels traversing the image."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "607 Sensory Coding Final Project",
    "section": "Limitations",
    "text": "Limitations\n\nMapping\nAs mentioned above, CNNs typically work by sliding a series of small kernels across an input image. The kernels learned by the network do take on similarities to receptive fields in the visual system, however they fail to capture the idea of a retinotopic map. Maps in the visual system are fixed, and visual stimuli “move across” them, which is precisely the opposite of how CNNs work."
  },
  {
    "objectID": "index.html#task",
    "href": "index.html#task",
    "title": "607 Sensory Coding Final Project",
    "section": "Task",
    "text": "Task\nInitially, I trained my networks to detect an object type (cross, elbow, tee, or line) and a rotation direction (clockwise or counterclockwise). The rotating stimuli filled the frame, and fed to the network 360 degrees at a time. This proved to be far too easy a task for the 3DCNN, so I got greedy and created a new set of stimulus images comprised of short rotating bars at one of 81 possible locations in the image. Rather than a full 360 degree turn, these bars only turned 10 degrees per stimulus, so that the network could be trained to predict their average orientation.\nThis task proved to be too difficult for my simple architecture, while I could get decent performance on rotation direction, 36 possible orientations were too much for the simple networks. I was able to get the model to perform above chance, but never above ~20% accuracy.\nI’m quite sure that the task is learnable, but not with the simple architecture I used in this project. I think that a combination of approaches should yield an accurate model: first I need to employ additional layers, second I need to scale my kernels to better fit the size of the features they are being trained on, and third, I need to improve my loss functions.\n\nOn Loss Functions\nI crippled my model by writing a loss function that weighed the accuracy of two predictions equally. If both predictions are easy, this isn’t a problem. But when I tried to train the model to predict both rotation direction (easy) and global orientation (hard) with their loss averaged, the model was unable to learn. My thought had been that training on both tasks at once would produce receptive fields that learned to specialize in one task or another. However the network has unable to get traction on either task.\nEventually it occurred to me that if I were to bias the network to learn one task over the other, it might help. By opting to weigh the rotation classification loss more heavily than the orientation loss, I was able to get the model learning again. Of course, this was the easy way out. –I believe another approach that would likely yield differentiated RFs and maintain better prediction accuracy for orientation would be to employ dropout, or to toggle between loss functions every other epoch. Either way, the more classes one has to predict on, the more deeper the network needs to be.\nBelow are classification results at loss functions weighing one task over the other. Clearly, orientation classification was the more difficult task of the two.\nNetwork with loss = rotation_loss + orientation_loss/4\nAccuracy of the network on rotation prediction: 89.53%\nAccuracy of the network on orientation prediction: 3.13%\n\nNetwork with loss = rotation_loss*0 + orientation_loss\nAccuracy of the network on rotation prediction: 50.42%\nAccuracy of the network on orientation prediction: 4.26%"
  },
  {
    "objectID": "bigrf.html#results",
    "href": "bigrf.html#results",
    "title": "Big Receptive Field",
    "section": "Results",
    "text": "Results\n\n\n\nFigure 4: One 16 channel layer of 8 total, each layer shows variations in grain structure."
  }
]